# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tcW3iQy5CbMWVyp8raA8JQZzDW1B5GPr
"""

!pip uninstall -y youtube-search-python httpx
!pip install youtube-search-python==1.6.4 httpx==0.23.0

!pip install playwright nest_asyncio
!playwright install
!pip install -q faiss-cpu sentence-transformers

from huggingface_hub import login
import torch
# 🔐 Login with your token
login("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")

# ✅ Install core libraries
!pip install -q transformers accelerate bitsandbytes
!pip install -U transformers accelerate

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# ✅ Qwen 1.5 model ID
model_id = "Qwen/Qwen1.5-7B-Chat"  # or use "Qwen/Qwen1.8-7B-Chat" if upgrading

# ✅ Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

# ✅ 8-bit quantization config + float16 for faster load/infer
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                         # ← switch to 4-bit
    bnb_4bit_compute_dtype=torch.float16,      # fast compute
    bnb_4bit_use_double_quant=True,            # better accuracy in 4-bit
    bnb_4bit_quant_type="nf4",                 # use `nf4` (default) or `fp4`
)

# ✅ Load model
model1 = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.float16,            # 💥 CRUCIAL for speed on 15 GB VRAM
    trust_remote_code=True,
)

# Load only once
# ✅ 1. Install if needed


# ✅ 2. Imports
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import re
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
bing_chunks, faiss_index = [], None

def planner(query):
    import asyncio
    import nest_asyncio
    from playwright.async_api import async_playwright
    import re

    nest_asyncio.apply()

    # 🧠 STEP 1: Scrape raw Bing visible text
    async def scrape_bing_visible_text(query):
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True, args=["--no-sandbox"])
            context = await browser.new_context()
            page = await context.new_page()
            print(f"🔍 Searching on Bing: {query}")
            await page.goto(f"https://www.bing.com/search?q={query}&setlang=en-us", timeout=70000)
            try:
                await page.click("text=Accept", timeout=2000)
            except:
                pass
            await page.mouse.wheel(0, 3000)
            await page.wait_for_timeout(2000)
            scraped_text = await page.evaluate("() => document.body.innerText")
            await browser.close()
            return scraped_text

    # ✂️ STEP 2: Chunk by day/section headers
    def chunk_trip_text(raw_text):
        split_regex = r"\b(?:Day\s+\d+|Morning|Afternoon|Evening|Night)\b"
        matches = list(re.finditer(split_regex, raw_text, flags=re.IGNORECASE))

        chunks, prev_end = [], 0
        for match in matches:
            end = match.start()
            chunk = raw_text[prev_end:end].strip()
            if len(chunk) > 50:
                chunks.append(chunk)
            prev_end = end

        final_chunk = raw_text[prev_end:].strip()
        if len(final_chunk) > 50:
            chunks.append(final_chunk)

        return chunks

    # 🧪 STEP 3: Scrape + Chunk + Return raw chunks
    async def run_trip_chunk_preview(query):
        raw_text = await scrape_bing_visible_text(query)
        chunks = chunk_trip_text(raw_text)
        print(f"\n✅ Total Chunks Found: {len(chunks)}")
        return chunks

    # 🌍 Format Prompt for LLM
    def format_trip_prompt(top_chunks):
        system_prompt = (
            "You are a travel planning assistant. Given real-world travel suggestions from Bing, "
            "summarize and convert it into a structured 3-day trip plan."
        )
        chunk_info = "\n\n".join([f"Chunk {i+1}:\n{chunk}" for i, chunk in enumerate(top_chunks)])
        prompt = (
            f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
            f"<|im_start|>user\nUse the following information to create a 3-day trip:\n\n{chunk_info}\n<|im_end|>\n"
            f"<|im_start|>assistant\n"
        )
        return prompt

    # 🧠 Generate summary using LLM
    def generate_trip_summary(prompt):
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
        input_len = inputs.input_ids.shape[1]

        output_ids = model1.generate(
            input_ids=inputs.input_ids,
            max_new_tokens=500,
            do_sample=True,
            top_p=0.9,
            temperature=0.85,
            pad_token_id=tokenizer.eos_token_id
        )

        generated = tokenizer.decode(output_ids[0][input_len:], skip_special_tokens=True)
        return generated.strip()

    # ✅ Final Pipeline Call
    chunks = asyncio.run(run_trip_chunk_preview(query))
    top_chunks = chunks[:6]  # Only use top 6 chunks

    trip_prompt = format_trip_prompt(top_chunks)
    final_summary = generate_trip_summary(trip_prompt)
    print(final_summary)

    return final_summary



import asyncio
import nest_asyncio
from playwright.async_api import async_playwright
def webscrape(query):
    import asyncio
    import nest_asyncio
    from playwright.async_api import async_playwright

    nest_asyncio.apply()

    async def scrape_visible_text(url):
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True, args=["--no-sandbox"])
            page = await browser.new_page()
            full_url = "https://" + url if not url.startswith("http") else url

            print(f"🌐 Navigating to: {full_url}")

            try:
                await page.goto(full_url, timeout=60000)
                await page.wait_for_timeout(3000)  # Wait for JS content to render
                visible_text = await page.evaluate("() => document.body.innerText")
                print(f"\n📄 Scraped Text Preview:\n{'-'*60}")
                print(visible_text[:1500])  # Show first 1500 chars for debug
                print(f"\n{'-'*60}\n✅ Scraping complete.")
            except Exception as e:
                print(f"❌ Error while scraping: {e}")
                visible_text = ""

            await browser.close()
            return visible_text

    async def generate_summary():
        text = await scrape_visible_text(query)

        if not text.strip():
            return "⚠️ No visible content found. Might be an empty or JS-only site."

        # ✅ Clean formatted Qwen prompt
        system_prompt = "You are a helpful assistant that summarizes web pages."
        full_prompt = (
            f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
            f"<|im_start|>user\nSummarize the following website content:\n\n{text}\n<|im_end|>\n"
            f"<|im_start|>assistant\n"
        )

        print("\n🧠 Generating summary using consistent format...\n")

        try:
            inputs = tokenizer(full_prompt, return_tensors="pt").to("cuda")
            input_ids = inputs.input_ids
            input_len = input_ids.shape[1]

            output_ids = model1.generate(
                input_ids=input_ids,
                max_new_tokens=500,
                do_sample=False,  # ← Greedy decoding
                temperature=1.0,  # Ignored when do_sample=False
                pad_token_id=tokenizer.eos_token_id
            )

            # ✅ Slice only the new output
            generated = tokenizer.decode(output_ids[0][input_len:], skip_special_tokens=True)
            print(generated)
            return generated.strip()

        except Exception as e:
            print(f"❌ Generation Error: {e}")
            return "⚠️ Failed to generate summary."

    return asyncio.run(generate_summary())

  # ✅ RUN the coroutine



# Define summary pipeline
def summary_pipeline(query):
    import nest_asyncio
    nest_asyncio.apply()

    import torch
    import requests
    import re
    from bs4 import BeautifulSoup
    from youtubesearchpython import VideosSearch
    from playwright.async_api import async_playwright

    def chunk_bing_by_time(raw_text):
        time_split_regex = r"\b\d+\s+(?:hours?|days?|minutes?)\s+ago\b"
        matches = list(re.finditer(time_split_regex, raw_text, flags=re.IGNORECASE))
        chunks, prev_end = [], 0
        for match in matches:
            end = match.end()
            chunk = raw_text[prev_end:end].strip()
            if len(chunk) > 50:
                chunks.append(chunk)
            prev_end = end
        if prev_end < len(raw_text):
            final_chunk = raw_text[prev_end:].strip()
            if len(final_chunk) > 50:
                chunks.append(final_chunk)
        return chunks

    def update_bing_index(raw_bing_text):
        global bing_chunks, faiss_index
        print("⚙️ Updating Bing FAISS index...")
        bing_chunks = chunk_bing_by_time(raw_bing_text)
        embeddings = embedding_model.encode(bing_chunks, show_progress_bar=True)
        dimension = embeddings[0].shape[0]
        faiss_index = faiss.IndexFlatL2(dimension)
        faiss_index.add(np.array(embeddings))
        print(f"✅ {len(bing_chunks)} chunks embedded and indexed.")

    def search_bing_chunks(query, top_k=3):
        if faiss_index is None:
            print("❌ FAISS index is not initialized.")
            return []
        query_vector = embedding_model.encode([query])
        D, I = faiss_index.search(np.array(query_vector), top_k)
        results = []
        for rank, idx in enumerate(I[0]):
            chunk = bing_chunks[idx]
            print(f"\n🎯 Top {rank+1} Chunk:\n{chunk}\n{'-'*50}")
            results.append(chunk)
        return results

    def format_qwen_prompt(user_query, top_chunks, youtube_links, hackernews_content):
        system_prompt = (
            "You are a helpful and informative assistant who summarizes data from multiple sources "
            "such as Bing search, YouTube, and Hacker News."
        )
        chunk1 = top_chunks[0] if len(top_chunks) > 0 else "No chunk available."
        chunk2 = top_chunks[1] if len(top_chunks) > 1 else "No second chunk available."
        info = f"""
📄 Relevant Info Chunks:
1. {chunk1}
2. {chunk2}

🎥 YouTube Summary (if any): {youtube_links or 'N/A'}
🟠 Hacker News Summary (if any): {hackernews_content[:1000] or 'N/A'}
"""
        prompt = (
            f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
            f"<|im_start|>user\n{user_query}\n{info}<|im_end|>\n"
            f"<|im_start|>assistant\n"
        )
        return prompt

    def def_generate(prompt, tokenizer, model1):
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids.cuda()
        output_ids = model1.generate(
            input_ids=input_ids,
            max_new_tokens=500,
            do_sample=True,
            top_p=0.9,
            temperature=0.85,
            pad_token_id=tokenizer.eos_token_id
        )
        generated_text = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)
        return generated_text.strip()

    async def scrape_bing_visible_text(query):
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context()
            page = await context.new_page()
            print(f"🔍 Searching on Bing: {query}")
            await page.goto(f"https://www.bing.com/search?q={query}&setlang=en-us", timeout=70000)
            try:
                await page.click("text=Accept", timeout=2000)
            except:
                pass
            await page.mouse.wheel(0, 3000)
            await page.wait_for_timeout(2000)
            scraped_text = await page.evaluate("() => document.body.innerText")
            await browser.close()
            return scraped_text

    def youtube_search(query):
        print(f"\n🎥 YouTube results for: {query}")
        results = VideosSearch(query, limit=3).result()
        top_links = []
        for video in results['result']:
            title = video['title']
            link = video['link']
            print(f"🎬 {title}\n🔗 {link}\n")
            top_links.append(link)
        return top_links

    def get_hn_top_link(query):
        print(f"\n🟠 Hacker News results for: {query}")
        url = f"https://hn.algolia.com/api/v1/search?query={query}"
        response = requests.get(url).json()
        hits = response['hits'][:10]
        for i, hit in enumerate(hits, 1):
            title = hit.get('title') or hit.get('story_title')
            link = hit.get('url') or f"https://news.ycombinator.com/item?id={hit['objectID']}"
            print(f"{i}. {title}\n   🔗 {link}\n")
            if link.startswith("http"):
                return link
        return None

    async def scrape_visible_text(url):
        async with async_playwright() as p:
            browser = await p.chromium.launch()
            page = await browser.new_page()
            await page.goto(url)
            await page.wait_for_timeout(3000)
            content = await page.content()
            await browser.close()
            soup = BeautifulSoup(content, 'html.parser')
            texts = soup.stripped_strings
            full_text = "\n".join(texts)
            return full_text

    async def run_pipeline(query):
        global bing_summary, youtube_links, hackernews_content
        bing_summary = ""
        youtube_links = []
        hackernews_content = ""
        bing_text = await scrape_bing_visible_text(query)
        youtube_links = youtube_search(query)
        top_hn_link = get_hn_top_link(query)
        if top_hn_link:
            hackernews_content = await scrape_visible_text(top_hn_link)
        else:
            print("⚠️ No valid Hacker News link found.")
        return bing_text

    import asyncio
    bing_text = asyncio.run(run_pipeline(query))
    update_bing_index(bing_text)
    top_chunks = search_bing_chunks(query)
    prompt = format_qwen_prompt(query, top_chunks, youtube_links, hackernews_content)
    response = def_generate(prompt, tokenizer, model1)
    print(response)

# 🔁 Outside loop — works with all intents
while True:
    query = input("\n🔍 Enter your topic (or 'exit' to quit): ")
    if query.lower() == "exit":
        break

    # 👉 Replace with actual intent extractor if needed
    if "www." in query.lower():
        intent= "webscraping"
    elif "plan" in query.lower():
        intent= "planner"
    else:
        intent= "summary" # you can make this dynamic
    if intent=="planner":
        planner(query)
    if intent == "summary":
        summary_pipeline(query)
    elif intent=="webscraping":
        webscrape(query)

